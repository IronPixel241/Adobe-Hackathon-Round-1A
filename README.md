#  PDF Outline Extractor – Challenge 1a Solution

## Overview

This project is a robust, high-performance solution for **Challenge 1a of the Adobe India Hackathon 2025**.  
The goal is to parse PDF documents and extract a **structured outline**, consisting of the **document's title** and a **hierarchical list of headings** (`H1`, `H2`, `H3`, etc.).

###  Key Features

-  Heuristic + Rule-Based approach for lightweight, fast execution.
-  Outputs structured JSON following Adobe’s defined schema.
-  Fully offline – operates without internet.
-  Supports multilingual documents (e.g., Japanese, English).
-  Built in Python, Docker-ready, architecture: `linux/amd64`.

---

##  How to Run

###  1. Build the Docker Image

```bash
docker build --platform linux/amd64 -t pdf-processor-1a .
```

### 2. Run the command
```bash
docker run --rm -v "${pwd}/sample_dataset/pdfs:/app/input:ro" -v "${pwd}/sample_dataset/outputs:/app/output" --network none pdf-processor-1a
```

## Directory Structure

```bash
Challenge_1a/
├── parser/                    
│   ├── block_extractor.py      # Extracts raw text blocks from PDF
│   ├── heuristics.py           # Filters out non-heading text using rules
│   ├── classifier.py           # Assigns heading levels (H1–H3)
│   ├── hierarchy_builder.py    # Builds final structured outline
│   └── config.py               # Configuration for font/heading thresholds
│
├── sample_dataset/            
│   ├── pdfs/                   # Input PDFs
│   └── outputs/                # Output JSONs generated by the model
│
├── process_pdfs.py             # Main pipeline script (entrypoint)
├── requirements.txt            # Python dependencies
├── Dockerfile                  # Docker build instructions
└── README.md                   # Project documentation
```

## Our Approach

To meet the challenge's constraints (speed, no network, no large models), we opted for a **Hybrid Heuristic and Rule-Based Pipeline**. This approach avoids heavy machine learning models in favor of a fast, deterministic, and highly accurate system that analyzes a document's structural and stylistic properties.

Our solution is modular and follows a four-stage pipeline:

### 1. Raw Block Extraction (`parser/block_extractor.py`)

The first step is to read the PDF and extract all text content. Instead of a simple text dump, we extract text lines as individual "blocks," each containing rich metadata:

- The text content itself  
- Font size, name, and weight (boldness)  
- Page number  

---

### 2. Heuristic Filtering (`parser/heuristics.py`)

This module filters noisy or irrelevant text blocks and isolates strong candidates likely to be structural headings. It applies three stages of document-aware heuristics:

- **Header/Footer Removal:** Removes repetitive lines from top/bottom regions that appear across multiple pages.

- **Length Filtering:** Discards lines that are too long or wordy to be headings.

- **Font Prominence Filtering:** Retains lines with larger or stylistically important font sizes.

---

### 3. Heading Classification (`parser/classifier.py`)

- **Structural Pattern Matching:** Detects numeric prefixes like "1." or "2.1.3" and maps them to heading levels.

- **Font-Size-Based Fallback:** Uses the top 3 font sizes in the document to infer heading levels when patterns are absent.

- **Sanity Checks:** Avoids classifying purely numeric or invalid short lines as headings.


---

### 4. Hierarchy Building (`parser/hierarchy_builder.py`)

The final module assembles the structured output:

- Identifies the document's **main title** (typically the largest text on the first page).  
- Organizes the classified blocks into a final JSON outline, ensuring no duplicates are included.

---

## Libraries and Models Used

Our solution is lightweight and relies on a minimal set of powerful, open-source libraries:

- [`PyMuPDF (fitz)`](https://pymupdf.readthedocs.io/en/latest/): A high-performance Python library for PDF parsing.
- `numpy`: Used for efficient numerical operations, specifically for calculating font size percentiles.

>  **Note:**  
> This solution is completely **model-free** – it uses **no machine learning models**.  
> **Ultra-fast**, **low memory**, and fits well within the **200MB offline constraint**.


---

